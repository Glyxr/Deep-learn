4.6 dropout
1.什么是好的模型
（1）简单性
（2）平滑性：对输入的微小变化不敏感（建议在计算后续层之前向网络的每一层注入噪声 - dropout）
2.（1）暂退法在前向传播过程中，计算每一内部层的同时丢弃一些神经元。
  （2）暂退法可以避免过拟合，它通常与控制权重向量的维数和大小结合使用的。
  （3）暂退法将活性值h替换为具有期望值的随机变量h_pred。
  （4）暂退法仅在训练期间使用。
3.dropout有什么作用？An:增强模型的平滑性
4.8
梯度爆炸：参数更新过大，破坏了模型的稳定收敛
梯度消失：参数更新过小，在每次更新时几乎不会移动，导致模型无法学习
y.backward(torch.ones_like(x))：https://blog.csdn.net/sinat_28731575/article/details/90342082
Distill：https://distill.pub/